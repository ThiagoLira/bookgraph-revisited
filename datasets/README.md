# Datasets

This directory contains the core datasets used by the Bookgraph Revisited pipeline.

## Files

### Goodreads Data
*   **`books_index.db`**: A SQLite database containing the Full Text Search (FTS5) index for Goodreads books.
    *   **Generated by**: `scripts/build_goodreads_index.py`
    *   **Input**: `goodreads_books.json` (and optionally `goodreads_book_authors.json` for denormalization)
    *   **Schema**: Contains `books_fts` table with `title`, `authors`, and JSON `data` payload.
*   **`goodreads_book_authors.json`**: JSON lines file containing author metadata (ID, name, ratings).
    *   **Source**: Raw dump from Goodreads (UCSD Book Graph dataset).
*   **`goodreads_books.json`**: JSON lines file containing book metadata.
    *   **Source**: Raw dump from Goodreads (UCSD Book Graph dataset).
    *   **Note**: This is the source for `books_index.db`.
*   **`authors_metadata.json`**: An override/augmentation file for specific authors.
    *   **Generated by**: `scripts/generate_author_metadata.py` (or manually edited).
    *   **Purpose**: Provides canonical birth/death years and other metadata that might be missing or messy in the raw dump.
*   **`original_publication_dates.json`**: A curated mapping of Goodreads Work/Book IDs to their **Original Publication Year**.
    *   **Problem**: Raw Goodreads data often only contains the year of the specific edition (e.g., 2003 for a 1914 edition). This breaks historical timelines.
    *   **Generated by**: The **Metadata Enricher** pipeline component.
        1.  **Cache**: Checks this file first.
        2.  **Scraper**: Live scrapes the Goodreads work page to find the true original date.
        3.  **LLM**: Falls back to LLM knowledge ("When was 'The Wealth of Nations' first published?").
    *   **Updates**: New findings from pipeline runs are automatically merged back into this file, making it a growing "Knowledge Base" of historical truth for the project.

### Wikipedia Data
*   **`wiki_people_index.db`**: A SQLite database containing an FTS index of Wikipedia people pages.
    *   **Generated by**: `scripts/build_wiki_people_index.py`.
    *   **Input**: `people_pages.jsonl` (which is filtered from the full Wikipedia dump via `scripts/filter_wiki_people.py`).
    *   **Purpose**: Used for resolving "Person" citations (e.g., philosophers, historical figures) that may not be authors of books in Goodreads.

## Usage
These files are expected to be in `datasets/` (formerly `datasets/`). The path is configurable in most scripts but defaults to this location.
